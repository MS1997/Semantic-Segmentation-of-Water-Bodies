{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Identify_water_bodies_reg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1xnJvIr15gLVYlbOcvSVWMWFnQ8EJ51Yr",
      "authorship_tag": "ABX9TyO9wXsW0F/J76OWsyzu+bcx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MS1997/Semantic-Segmentation-of-Water-Bodies/blob/master/Identify_water_bodies_reg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPw5tdoiLFKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from PIL import Image\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# setting all the seeds  \n",
        "SEED = 101\n",
        "import os\n",
        "import shutil\n",
        "import random as rn\n",
        "os.environ['PYTHONHASHSEED']=str(SEED)\n",
        "np.random.seed(SEED)\n",
        "# tf.random.set_seed(SEED)\n",
        "rn.seed(SEED)\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.layers import Conv2D, concatenate, Conv2DTranspose, Input, MaxPooling2D, ReLU, BatchNormalization, Dense\n",
        "from keras.optimizers import Adam\n",
        "from random import sample \n",
        "from keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgPMCFOnNPKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # installing kaggle library to import the data directly into Colab notebook\n",
        " ! pip install -q kaggle"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1lhxxDHK-EI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select file containing API key from local system\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rEQbKF04nvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE6Y4Jah4p1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing the chest x-ray data set from kaggle\n",
        "! kaggle datasets download -d franciscoescobar/satellite-images-of-water-bodies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ztmj94NIL7lv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unzipping the files\n",
        "! unzip satellite-images-of-water-bodies.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLBILAiqLKW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a folder for validation set of image and masks \n",
        "os.mkdir('validation')\n",
        "os.mkdir('validation/Images')\n",
        "os.mkdir('validation/Images/Images')\n",
        "os.mkdir('validation/Masks')\n",
        "os.mkdir('validation/Masks/Masks')\n",
        "\n",
        "# to create generator objects from directory, we will need to make another folder \n",
        "os.mkdir('Water Bodies Dataset/Images/Images')\n",
        "os.mkdir('Water Bodies Dataset/Masks/Masks')\n",
        "\n",
        "# changing the location of the Images  \n",
        "for i in os.listdir('Water Bodies Dataset/Images'):\n",
        "  if i.endswith('.jpg'):\n",
        "    os.rename('Water Bodies Dataset/Images/'+i,'Water Bodies Dataset/Images/Images/'+i)\n",
        "# changing the location of the Masks  \n",
        "for i in os.listdir('Water Bodies Dataset/Masks'):\n",
        "  if i.endswith('.jpg'):\n",
        "    os.rename('Water Bodies Dataset/Masks/'+i,'Water Bodies Dataset/Masks/Masks/'+i)\n",
        "\n",
        "s = sample(range(0,2841),281)\n",
        "\n",
        "img_list = sorted(os.listdir('Water Bodies Dataset/Images/Images'))\n",
        "mask_list = sorted(os.listdir('Water Bodies Dataset/Masks/Masks'))\n",
        "\n",
        "val_image = [img_list[i] for i in s]\n",
        "val_mask = [mask_list[i] for i in s]\n",
        "\n",
        "# getting validation images and their masks  \n",
        "for i in os.listdir('Water Bodies Dataset/Images/Images'):\n",
        "  if i in val_image:\n",
        "    os.rename('Water Bodies Dataset/Images/Images/'+i,'validation/Images/Images/'+i)\n",
        "for i in os.listdir('Water Bodies Dataset/Masks/Masks'):\n",
        "  if i in val_mask:\n",
        "    os.rename('Water Bodies Dataset/Masks/Masks/'+i,'validation/Masks/Masks/'+i)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqjWbCMNhFv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a folder for test set of image and masks \n",
        "os.mkdir('test')\n",
        "os.mkdir('test/Images')\n",
        "os.mkdir('test/Images/Images')\n",
        "os.mkdir('test/Masks')\n",
        "os.mkdir('test/Masks/Masks')\n",
        "\n",
        "s = sample(range(0,281),25)\n",
        "\n",
        "img_list = sorted(os.listdir('validation/Images/Images'))\n",
        "mask_list = sorted(os.listdir('validation/Masks/Masks'))\n",
        "\n",
        "test_image = [img_list[i] for i in s]\n",
        "test_mask = [mask_list[i] for i in s]\n",
        "\n",
        "# getting validation images and their masks  \n",
        "for i in os.listdir('validation/Images/Images/'):\n",
        "  if i in test_image:\n",
        "    os.rename('validation/Images/Images/'+i,'test/Images/Images/'+i)\n",
        "for i in os.listdir('validation/Masks/Masks'):\n",
        "  if i in test_mask:\n",
        "    os.rename('validation/Masks/Masks/'+i,'test/Masks/Masks/'+i)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2AUGAIxLW4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# directory to store augmented images \n",
        "os.mkdir('Water Bodies Dataset/augmented')\n",
        "\n",
        "os.mkdir('Water Bodies Dataset/augmented/Images')\n",
        "os.mkdir('Water Bodies Dataset/augmented/Masks')"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REzeYU0h91bM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shutil.rmtree('Water Bodies Dataset/augmented')"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAw0OlhF97fO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_here_img = 'Water Bodies Dataset/augmented/Images'\n",
        "save_here_msk = 'Water Bodies Dataset/augmented/Masks'\n",
        "\n",
        "# create two instances with the same arguments\n",
        "data_gen_args = dict(rescale=1.0/255.0 ) # base case\n",
        "#horizontal_flip = True, vertical_flip = True - case 2\n",
        "#horizontal_flip = True, vertical_flip = True, rotation_range = 20 - case 2\n",
        "#width_shift_range =0.3, height_shift_range = 0.3 - case 4\n",
        "# shear_range=20, zoom_range=[0.2, 0.5] - case 5\n",
        "\n",
        "image_datagen = ImageDataGenerator(**data_gen_args)\n",
        "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "# using same seed and keyword arguments to the fit and flow methods\n",
        "seed = 101\n",
        "image_generator = image_datagen.flow_from_directory('Water Bodies Dataset/Images', class_mode=None,seed=seed, target_size=(256, 256), batch_size= 16, save_to_dir=save_here_img, save_prefix = 'aug', save_format= 'jpg')\n",
        "mask_generator = mask_datagen.flow_from_directory('Water Bodies Dataset/Masks',class_mode=None, seed=seed,target_size=(256, 256), batch_size=16, color_mode='grayscale',save_to_dir= save_here_msk, save_prefix='aug', save_format= 'jpg')\n",
        "# color_mode = 'grayscale' for masks since they are black and white\n",
        "\n",
        "# combine generators into one which yields image and masks\n",
        "train_generator = zip(image_generator, mask_generator)\n",
        "next(train_generator) # to save the first set of augmentations so that we can see if they are correct \n",
        "\n",
        "# validation generator\n",
        "val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "val_image_generator = val_datagen.flow_from_directory('validation/Images',target_size=(256, 256),batch_size=16,class_mode=None, seed = seed)\n",
        "val_mask_generator = val_datagen.flow_from_directory('validation/Masks',target_size=(256, 256),batch_size=16,class_mode=None, seed = seed, color_mode='grayscale')\n",
        "val_generator = zip(val_image_generator, val_mask_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u3izEy29-bc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l_mask = os.listdir('Water Bodies Dataset/augmented/Masks')\n",
        "len(os.listdir('Water Bodies Dataset/augmented/Masks'))\n",
        "\n",
        "l_image = os.listdir('Water Bodies Dataset/augmented/Images')\n",
        "os.listdir('Water Bodies Dataset/augmented/Images')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNW7drrm-EOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modelling \n",
        "# we will use keras functional API \n",
        "# encoder \n",
        "# Input \n",
        "input_img = Input(shape=(256, 256, 3))\n",
        "# Encoder \n",
        "c1 = Conv2D(32, (3, 3),padding='same')(input_img)\n",
        "c1 = BatchNormalization()(c1)\n",
        "c1 = ReLU()(c1)\n",
        "c1 = Conv2D(32, (3, 3),padding='same')(c1)\n",
        "c1 = BatchNormalization()(c1)\n",
        "c1 = ReLU()(c1)\n",
        "p1 = MaxPooling2D((2,2))(c1)\n",
        "\n",
        "c2 = Conv2D(64, (3, 3),padding='same')(p1)\n",
        "c2 = BatchNormalization()(c2)\n",
        "c2 = ReLU()(c2)\n",
        "c2 = Conv2D(64, (3, 3),padding='same')(c2)\n",
        "c2 = BatchNormalization()(c2)\n",
        "c2 = ReLU()(c2)\n",
        "p2 = MaxPooling2D((2,2))(c2)\n",
        "\n",
        "c3 = Conv2D(128, (3, 3),padding='same')(p2)\n",
        "c3 = BatchNormalization()(c3)\n",
        "c3 = ReLU()(c3)\n",
        "c3 = Conv2D(128, (3, 3),padding='same')(c3)\n",
        "c3 = BatchNormalization()(c3)\n",
        "c3 = ReLU()(c3)\n",
        "p3 = MaxPooling2D((2,2))(c3)\n",
        "\n",
        "c4 = Conv2D(256, (3, 3),padding='same')(p3)\n",
        "c4 = BatchNormalization()(c4)\n",
        "c4 = ReLU()(c4)\n",
        "c4 = Conv2D(256, (3, 3),padding='same')(c4)\n",
        "c4 = BatchNormalization()(c4)\n",
        "c4 = ReLU()(c4)\n",
        "p4 = MaxPooling2D((2,2))(c4)\n",
        "\n",
        "c5 = Conv2D(512, (3, 3),padding='same')(p4)\n",
        "c5 = BatchNormalization()(c5)\n",
        "c5 = ReLU()(c5)\n",
        "c5 = Conv2D(512, (3, 3),padding='same')(c5)\n",
        "c5 = BatchNormalization()(c5)\n",
        "c5 = ReLU()(c5)\n",
        "p5 = MaxPooling2D((2,2))(c5)\n",
        "\n",
        "c6 = Conv2D(1024, (3, 3),padding='same')(p5)\n",
        "c6 = BatchNormalization()(c6)\n",
        "c6 = ReLU()(c6)\n",
        "c6 = Conv2D(1024, (3, 3),padding='same')(c6)\n",
        "c6 = BatchNormalization()(c6)\n",
        "c6 = ReLU()(c6)\n",
        "\n",
        "# Decoder\n",
        "t1 = Conv2DTranspose(512, (3,3),strides=(2,2),padding='same')(c6)\n",
        "t1 = concatenate([t1, c5]) \n",
        "t1 = BatchNormalization()(t1)\n",
        "t1 = ReLU()(t1)\n",
        "t1 = Conv2D(512, (3,3),padding='same')(t1)\n",
        "t1 = BatchNormalization()(t1)\n",
        "t1 = ReLU()(t1)\n",
        "t1 = Conv2D(512, (3,3),padding='same')(t1)\n",
        "t1 = BatchNormalization()(t1)\n",
        "t1 = ReLU()(t1)\n",
        "\n",
        "t2 = Conv2DTranspose(256, (3,3),strides=(2,2),padding='same')(t1)\n",
        "t2 = concatenate([t2, c4])\n",
        "t2 = BatchNormalization()(t2)\n",
        "t2 = ReLU()(t2)\n",
        "t2 = Conv2D(256, (3,3),padding='same')(t2)\n",
        "t2 = BatchNormalization()(t2)\n",
        "t2 = ReLU()(t2)\n",
        "t2 = Conv2D(256, (3,3),padding='same')(t2)\n",
        "t2 = BatchNormalization()(t2)\n",
        "t2 = ReLU()(t2)\n",
        "\n",
        "t3 = Conv2DTranspose(128, (3,3),strides=(2,2),padding='same')(t2)\n",
        "t3 = concatenate([t3, c3])\n",
        "t3 = BatchNormalization()(t3)\n",
        "t3 = ReLU()(t3)\n",
        "t3 = Conv2D(128, (3,3),padding='same')(t3)\n",
        "t3 = BatchNormalization()(t3)\n",
        "t3 = ReLU()(t3)\n",
        "t3 = Conv2D(128, (3,3),padding='same')(t3)\n",
        "t3 = BatchNormalization()(t3)\n",
        "t3 = ReLU()(t3)\n",
        "\n",
        "\n",
        "t4 = Conv2DTranspose(64, (3,3),strides=(2,2),padding='same')(t3)\n",
        "t4 = concatenate([t4, c2])\n",
        "t4 = BatchNormalization()(t4)\n",
        "t4 = ReLU()(t4)\n",
        "t4 = Conv2D(64, (3,3),padding='same')(t4)\n",
        "t4 = BatchNormalization()(t4)\n",
        "t4 = ReLU()(t4)\n",
        "t4 = Conv2D(64, (3,3),padding='same')(t4)\n",
        "t4 = BatchNormalization()(t4)\n",
        "t4 = ReLU()(t4)\n",
        "\n",
        "t5 = Conv2DTranspose(32, (3,3),strides=(2,2),padding='same')(t4)\n",
        "t5 = concatenate([t5, c1])\n",
        "t5 = BatchNormalization()(t5)\n",
        "t5 = ReLU()(t5)\n",
        "t5 = Conv2D(32, (3,3),padding='same')(t5)\n",
        "t5 = BatchNormalization()(t5)\n",
        "t5 = ReLU()(t5)\n",
        "t5 = Conv2D(32, (3,3),padding='same')(t5)\n",
        "t5 = BatchNormalization()(t5)\n",
        "t5 = ReLU()(t5)\n",
        "\n",
        "y = Conv2DTranspose(1, (1,1), activation='sigmoid')(t5)\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YUoayYz-Iu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_1 = Model(input_img,y)\n",
        "model_2 = Model(input_img,y)\n",
        "model_3 = Model(input_img,y)\n",
        "model_4 = Model(input_img,y)\n",
        "model_5 = Model(input_img,y)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yx1k4ZGYoQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint(\"/content/gdrive/My Drive/mw.h5\", save_weights_only = False, period = 20) # save model weights after 20 epochs\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FekiYZJYhd5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss= 'binary_crossentropy', optimizer=keras.optimizers.Adam(0.00001, beta_1=0.99, beta_2=0.99), metrics=['accuracy']) #keras.optimizers.Adam(0.000001)\n",
        "history_1 = model.fit_generator(train_generator,validation_data= val_generator, epochs=50,steps_per_epoch=160,validation_steps=16,callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri1Hv24RGNKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mounting the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky4yFBSBmJox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# convert the history.history dict to a pandas DataFrame:     \n",
        "hist_df = pd.DataFrame(history_1.history) \n",
        "\n",
        "\n",
        "# or save to csv: \n",
        "hist_csv_file = '/content/gdrive/My Drive/water_history.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSP_YxdPGyrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"/content/gdrive/My Drive/model_water.h5\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdES_Me2o_s6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_1= keras.models.load_model('/content/gdrive/My Drive/model_water_rescale_only_3.h5')\n",
        "model_2= keras.models.load_model('/content/gdrive/My Drive/model_water_207_part3.h5')\n",
        "model_3 = keras.models.load_model('/content/gdrive/My Drive/Copy of model_water_rot_3.h5')\n",
        "model_4 = keras.models.load_model('/content/gdrive/My Drive/Copy of model_water_shift_3.h5')\n",
        "model_5 = keras.models.load_model('/content/gdrive/My Drive/model_s_z_3.h5')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kdeum6hpHEI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# making predictions \n",
        "\n",
        "path_to_image = ''\n",
        "img = load_img(path_to_image)\n",
        "x = img_to_array(img)\n",
        "x = tf.expand_dims(x, axis=0)\n",
        "x=x/255 # rescale the test image\n",
        "\n",
        "probs = model.predict(x,steps=100)\n",
        "\n",
        "preds = (probs > 0.5).astype(int)\n",
        "\n",
        "# predicted image \n",
        "array_to_img(preds[99])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}